{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "D3LhGvjpmsIU"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiqeT2aCtnGf",
        "outputId": "b3bc3d40-a43b-4a3a-c82d-1358bdb2aabd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "HRIb4XYAk8Fo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92251395-8388-41c4-c85d-f650fca9eb5c",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.36.1-py3-none-any.whl (328 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/328.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m256.0/328.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m328.8/328.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.36.1\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import tiktoken\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "A2YaDedCk5Bc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab/NER\""
      ],
      "metadata": {
        "id": "Em2Uw2dRmFBB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate price using tiktoken"
      ],
      "metadata": {
        "id": "D3LhGvjpmsIU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get total paragraphs"
      ],
      "metadata": {
        "id": "rizCuHfvtStB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        return file.read()"
      ],
      "metadata": {
        "id": "mEfj1ERdtVsZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = f\"{BASE_PATH}/wikipedia_summary.txt\"\n",
        "text = read_file(filepath)\n",
        "paragraphs = text.replace(\"\\n\\n\", \"\\n\").split(\"\\n\")\n",
        "print(f\"The file contains {len(paragraphs)} paragraphs.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98qI6VY4tbL6",
        "outputId": "56332d5a-a652-402b-d8ad-c7a8dda31ecc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file contains 110038 paragraphs.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate price\n",
        "\n",
        "[Pricing OpenAI](https://openai.com/api/pricing/)"
      ],
      "metadata": {
        "id": "L5mhVwa9u0eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pricing in USD\n",
        "model_pricing_by_1K_tokens = {\n",
        "  \"gpt-4o-mini\": {\n",
        "    \"input_tokens\": {\n",
        "        \"api\": 0.00015,\n",
        "        \"bacth_api\": 0.00008\n",
        "    },\n",
        "    \"output_tokens\": {\n",
        "        \"api\": 0.0006,\n",
        "        \"bacth_api\": 0.0003\n",
        "    }\n",
        "  },\n",
        "  \"gpt-4o\": {\n",
        "    \"input_tokens\": {\n",
        "        \"api\": 0.005,\n",
        "        \"bacth_api\": 0.0025\n",
        "    },\n",
        "    \"output_tokens\": {\n",
        "        \"api\": 0.015,\n",
        "        \"bacth_api\": 0.0075\n",
        "    }\n",
        "  },\n",
        "}\n"
      ],
      "metadata": {
        "id": "j2RITvlVpChN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens_per_text(text, model):\n",
        "  try:\n",
        "      encoding = tiktoken.encoding_for_model(model)\n",
        "  except KeyError:\n",
        "      print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
        "      encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "  token_integers = encoding.encode(text)\n",
        "  print(f\"Model: {model}\\nEncoding: {encoding.name}\")\n",
        "  return len(token_integers)"
      ],
      "metadata": {
        "id": "xUQW594snGZa"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_cost(num_tokens, fee_per_token, tokens=1000):\n",
        "  final_price = (num_tokens * fee_per_token) / tokens\n",
        "  return final_price"
      ],
      "metadata": {
        "id": "Fqfx__gfnJT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gpt-4o-mini\"\n",
        "model_pricing = model_pricing_by_1K_tokens[model]\n",
        "batch_format_sample = '{\"custom_id\": \"wk-499\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-4o-mini\", \"messages\": [{\"role\": \"system\", \"content\": \"Eres un experto generando y creando etiquetas para un texto en espa√±ol.\"}, {\"role\": \"user\", \"content\": \"Como especialista anotador de informaci√≥n, tu tarea es detectar entidades de todo tipo en un texto para generar un dataset de entrenamiento de un modelo de IA.\\nEtiqueta todo tipo de entidades, conceptos e ideas como te sea posible, sali√©ndote del esquema general de los datasets NER.Sientete libre de crear nuevos tipos de entidades que pueden no existir en tareas NER tradicionales.\\nEl formato de salida es el siguiente: <palabra> <> <nombre de la entidad> <> <descripcion de la entidad>\\nEjemplo: Espa√±a <> Pa√≠s <> Naci√≥n localizada en el sur de EuropaEl texto es el siguiente: Elvira Malagarriga Armart, tambi√©n conocida como Elvira Ormart, fue una pintora espa√±ola especializada en retratos, paisajes y flores. Nacida en Barcelona en 1886, estudi√≥ en la Escuela de Bellas Artes de La Llotja y luego se form√≥ en Par√≠s, donde asisti√≥ a la Acad√©mie Julian. Malagarriga expuso su obra en diversos lugares de Espa√±a y Francia, incluyendo el Sal√≥n de Par√≠s, la Asociaci√≥n de Artistas Italianos en Florencia, y en Sala Par√©s en Barcelona. Su obra incluye retratos, como el de su padre y su hermana, as√≠ como paisajes y flores. Su estilo fue elogiado por la revista Feminal, que compar√≥ su habilidad con la de otras pintoras femeninas de la √©poca. El Museo Nacional de Arte de Catalu√±a conserva una de sus obras, un paseo con una casa se√±orial.\"}], \"max_tokens\": 2500}}'\n",
        "\n",
        "if model_pricing:\n",
        "  num_tokens = num_tokens_per_text(batch_format_sample, model)\n",
        "  print(f\"The text contains: {num_tokens} tokens.\")\n",
        "\n",
        "  # Estimate cost\n",
        "  input_price_with_api = estimate_cost(num_tokens, model_pricing[\"input_tokens\"][\"api\"])\n",
        "  print(f\"Estimated cost with API per paragraph: {input_price_with_api:2f}$.\")\n",
        "\n",
        "  input_price_with_api_per_100k_texts = input_price_with_api * len(paragraphs)\n",
        "  print(f\"Estimated cost with API per {len(paragraphs)} paragraphs: {input_price_with_api_per_100k_texts:2f}$.\")\n",
        "\n",
        "  input_price_with_batch_api = estimate_cost(num_tokens, model_pricing[\"input_tokens\"][\"bacth_api\"])\n",
        "  print(f\"Estimated cost with Batch API per paragraph: {input_price_with_batch_api:2f}$.\")\n",
        "\n",
        "  input_price_with_batch_api_per_100k_texts = input_price_with_batch_api * len(paragraphs)\n",
        "  print(f\"Estimated cost with Batch API per {len(paragraphs)} paragraphs: {input_price_with_batch_api_per_100k_texts:2f}$.\")\n",
        "else:\n",
        "  print(f\"Model {model} not found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1JPgLaAnMPf",
        "outputId": "65c35319-bf24-4a9c-c799-3065a2b50ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "The text contains: 396 tokens.\n",
            "Estimated cost with API per paragraph: 0.000059$.\n",
            "Estimated cost with API per 110038 paragraphs: 6.536257$.\n",
            "Estimated cost with Batch API per paragraph: 0.000032$.\n",
            "Estimated cost with Batch API per 110038 paragraphs: 3.486004$.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4XNbcXCuFb85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate NER tags"
      ],
      "metadata": {
        "id": "3sfwXohaBQvz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create promt for generate NER tags"
      ],
      "metadata": {
        "id": "os9FOqTP8QWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Run a test for the prompt***"
      ],
      "metadata": {
        "id": "iDLj6iM3BnR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = paragraphs[45]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  messages=[\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"Eres un experto generando y creando etiquetas para un texto en espa√±ol.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": (\n",
        "            \"Como especialista anotador de informaci√≥n, tu tarea es detectar entidades de todo tipo en un texto para generar un dataset de entrenamiento de un modelo de IA.\\n\"\n",
        "            \"Etiqueta todo tipo de entidades, conceptos e ideas como te sea posible, sali√©ndote del esquema general de los datasets NER.\"\n",
        "            \"Sientete libre de crear nuevos tipos de entidades que pueden no existir en tareas NER tradicionales.\\n\"\n",
        "            \"Muy importante: NO etiquetes palabras comunes como art√≠culos ('El', 'La', 'Los', 'Las'). En su lugar, etiqueta la palabra o frase a la que hace referencia el art√≠culo.\\n\"\n",
        "            \"El formato de salida es el siguiente: <palabra> <> <nombre de la entidad> <> <descripcion de la entidad>\"\n",
        "            \"Ejemplo: Espa√±a <> Pa√≠s <> Naci√≥n localizada en el sur de Europa.\"\n",
        "            f\"El texto es el siguiente: {sample_text}\"\n",
        "        ),\n",
        "    },\n",
        "  ],\n",
        "  temperature=0.7,\n",
        "  max_tokens=1024,\n",
        "  top_p=0.7,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0\n",
        ")\n",
        "\n",
        "response.choices[0].message.content.replace(\"\\n\\n\", \"\\n\").split(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7vPdNw88dza",
        "outputId": "7027b05b-0d47-4f4d-81c4-db05b8d04d99"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['√°lbum <> Obra musical <> Colecci√≥n de canciones grabadas por un artista o banda.  ',\n",
              " 'En Directo <> T√≠tulo de √°lbum <> Nombre del √°lbum grabado por la banda Cicatriz.  ',\n",
              " 'banda <> Grupo musical <> Conjunto de m√∫sicos que tocan juntos.  ',\n",
              " 'Cicatriz <> Banda de rock <> Grupo musical de rock originario de Espa√±a.  ',\n",
              " '1994 <> A√±o <> A√±o en el que fue grabado el √°lbum.  ',\n",
              " '1995 <> A√±o <> A√±o en el que fue lanzado el √°lbum.  ',\n",
              " 'homenaje <> Concepto <> Acto de recordar y honrar a alguien.  ',\n",
              " 'miembros <> Personas <> Integrantes de la banda Cicatriz.  ',\n",
              " 'fallecidos <> Estado <> Personas que han muerto.  ',\n",
              " 'energ√≠a <> Concepto <> Vitalidad y fuerza que transmite la m√∫sica.  ',\n",
              " 'sonido metalero <> G√©nero musical <> Estilo musical caracterizado por guitarras el√©ctricas distorsionadas y ritmos intensos.  ',\n",
              " 'guitarrista <> M√∫sico <> Persona que toca la guitarra.  ',\n",
              " 'Goar I√±urrieta <> Guitarrista <> Nombre del guitarrista de la banda Cicatriz.  ',\n",
              " 'estilo heavy <> Estilo musical <> Subg√©nero del rock caracterizado por su sonido fuerte y agresivo.  ',\n",
              " 'canciones punk <> G√©nero musical <> Composiciones musicales del estilo punk, caracterizadas por su energ√≠a y actitud rebelde.  ',\n",
              " 'intensidad <> Concepto <> Grado de fuerza y emoci√≥n en las actuaciones.  ',\n",
              " 'actuaciones en vivo <> Evento musical <> Presentaciones en directo ante un p√∫blico.  ',\n",
              " 'formaci√≥n original <> Estructura <> Conjunto inicial de miembros de la banda Cicatriz.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Batch API"
      ],
      "metadata": {
        "id": "szLeWrLYzaZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Batch file\n",
        "- Create a Jsonl file.\n",
        "- Each request must include a unique custom_id value, which you can use to reference results after completion\n",
        "\n",
        "Example:\n",
        "\n",
        "\n",
        "```\n",
        "{\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-3.5-turbo-0125\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},{\"role\": \"user\", \"content\": \"Hello world!\"}],\"max_tokens\": 1000}}\n",
        "{\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-3.5-turbo-0125\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an unhelpful assistant.\"},{\"role\": \"user\", \"content\": \"Hello world!\"}],\"max_tokens\": 1000}}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ANvwfsiB7psv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare global variables\n",
        "method = \"POST\"\n",
        "url = \"/v1/chat/completions\"\n",
        "model = \"gpt-4o-mini\"\n",
        "max_tokens = 2500\n",
        "system_prompt = \"Eres un experto generando y creando etiquetas para un texto en espa√±ol.\"\n",
        "chunk_size = 500\n",
        "limit_token = 200000\n",
        "batch_group = {}"
      ],
      "metadata": {
        "id": "RU6DdVSLFOIh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_text_in_jsonl(texts, file_path):\n",
        "  with open(file_path, \"w\", encoding='utf-8') as json_file:\n",
        "    for text in texts:\n",
        "        json_line = json.dumps(text, ensure_ascii=False)\n",
        "        json_file.write(json_line + '\\n')"
      ],
      "metadata": {
        "id": "hTOk1yyoJQrP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(paragraphs), chunk_size):\n",
        "  batch_format = []\n",
        "  chunk = paragraphs[i: i + chunk_size]\n",
        "  batch_name = f\"batch_{i}\"\n",
        "  batch_group[batch_name] = []\n",
        "\n",
        "  for index, paragraph in enumerate(chunk):\n",
        "    # To exclude phrases that are not article summaries\n",
        "    if len(paragraph) > 70:\n",
        "      batch_group[batch_name].append(paragraph)\n",
        "      custom_id = f\"wk-{index}\"\n",
        "      message = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_prompt\n",
        "        },\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": (\n",
        "            \"Como especialista anotador de informaci√≥n, tu tarea es detectar entidades de todo tipo en un texto para generar un dataset de entrenamiento de un modelo de IA.\\n\"\n",
        "            \"Etiqueta todo tipo de entidades, conceptos e ideas como te sea posible, sali√©ndote del esquema general de los datasets NER.\"\n",
        "            \"Sientete libre de crear nuevos tipos de entidades que pueden no existir en tareas NER tradicionales.\\n\"\n",
        "            \"Muy importante: NO etiquetes palabras comunes como art√≠culos ('El', 'La', 'Los', 'Las'). En su lugar, etiqueta la palabra o frase a la que hace referencia el art√≠culo.\\n\"\n",
        "            \"El formato de salida es el siguiente: <palabra> <> <nombre de la entidad> <> <descripcion de la entidad>\"\n",
        "            \"Ejemplo: Espa√±a <> Pa√≠s <> Naci√≥n localizada en el sur de Europa.\"\n",
        "            f\"\\nEl texto es el siguiente: {paragraph}\"\n",
        "          )\n",
        "        }\n",
        "      ]\n",
        "      batch_format.append({\n",
        "        \"custom_id\": custom_id,\n",
        "        \"method\": method,\n",
        "        \"url\": url,\n",
        "        \"body\": {\n",
        "          \"model\": model,\n",
        "          \"messages\": message,\n",
        "          \"max_tokens\": max_tokens\n",
        "        }\n",
        "      })\n",
        "\n",
        "  # total_tokens = num_tokens_per_text(str(batch_format), model)\n",
        "  # if total_tokens > limit_token:\n",
        "  #   print(f\"üí•The token amount is greater than that allowed by {model}. Total tokens: {total_tokens}\")\n",
        "  #   break\n",
        "\n",
        "  filename = f\"batch_{i}.jsonl\"\n",
        "  filepath = f\"{BASE_PATH}/{filename}\"\n",
        "\n",
        "  save_text_in_jsonl(batch_format, filepath)\n",
        "  print(f\"Saved file: {filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HupG7Ck7hYi",
        "outputId": "2383af64-d741-4055-f557-13a206b04c2d",
        "collapsed": true
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_0.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_1000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_1500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_2000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_2500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_3000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_3500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_4000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_4500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_5000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_5500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_6000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_6500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_7000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_7500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_8000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_8500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_9000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_9500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_10000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_10500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_11000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_11500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_12000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_12500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_13000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_13500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_14000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_14500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_15000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_15500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_16000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_16500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_17000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_17500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_18000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_18500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_19000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_19500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_20000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_20500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_21000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_21500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_22000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_22500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_23000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_23500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_24000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_24500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_25000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_25500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_26000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_26500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_27000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_27500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_28000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_28500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_29000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_29500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_30000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_30500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_31000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_31500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_32000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_32500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_33000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_33500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_34000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_34500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_35000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_35500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_36000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_36500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_37000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_37500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_38000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_38500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_39000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_39500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_40000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_40500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_41000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_41500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_42000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_42500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_43000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_43500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_44000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_44500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_45000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_45500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_46000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_46500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_47000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_47500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_48000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_48500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_49000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_49500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_50000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_50500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_51000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_51500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_52000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_52500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_53000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_53500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_54000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_54500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_55000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_55500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_56000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_56500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_57000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_57500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_58000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_58500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_59000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_59500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_60000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_60500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_61000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_61500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_62000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_62500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_63000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_63500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_64000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_64500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_65000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_65500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_66000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_66500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_67000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_67500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_68000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_68500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_69000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_69500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_70000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_70500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_71000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_71500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_72000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_72500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_73000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_73500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_74000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_74500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_75000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_75500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_76000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_76500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_77000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_77500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_78000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_78500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_79000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_79500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_80000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_80500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_81000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_81500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_82000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_82500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_83000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_83500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_84000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_84500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_85000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_85500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_86000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_86500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_87000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_87500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_88000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_88500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_89000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_89500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_90000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_90500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_91000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_91500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_92000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_92500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_93000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_93500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_94000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_94500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_95000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_95500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_96000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_96500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_97000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_97500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_98000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_98500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_99000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_99500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_100000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_100500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_101000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_101500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_102000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_102500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_103000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_103500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_104000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_104500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_105000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_105500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_106000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_106500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_107000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_107500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_108000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_108500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_109000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_109500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_110000.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload Batch file"
      ],
      "metadata": {
        "id": "UJPcEc537v1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jsonl_files = [\n",
        "  f\"{BASE_PATH}/{file}\"\n",
        "  for file in os.listdir(BASE_PATH)\n",
        "  if file.endswith(\".jsonl\")\n",
        "]\n",
        "len(jsonl_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6yl8oY0PUwEf",
        "outputId": "8dccbfeb-b79e-42e8-c7ca-f07d99bd66a8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "222"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Do the following steps for the number of jsonl files"
      ],
      "metadata": {
        "id": "nnki5Wp3bFCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_name = \"batch_0\"\n",
        "jsonl_file = f\"{batch_name}.jsonl\""
      ],
      "metadata": {
        "id": "oCtquhZzrq4w"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_file = client.files.create(\n",
        "  file=open(f\"{BASE_PATH}/{jsonl_file}\", \"rb\"),\n",
        "  purpose=\"batch\"\n",
        ")\n",
        "\n",
        "batch_file_id = batch_file.id\n",
        "print(f\"Uploaded: {jsonl_file} \\nFile id: {batch_file_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlAqVcTzUWQo",
        "outputId": "c18ac57d-ad7b-4be2-f1c7-e47dd6bd80f2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded: batch_0.jsonl \n",
            "File id: file-w6JIa9w7fF4B4YIqgZzw3GGH\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the batch job\n",
        "üí°One job must be created at a time"
      ],
      "metadata": {
        "id": "nrjJ7vJl7udM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_response = client.batches.create(\n",
        "  input_file_id=batch_file_id,\n",
        "  endpoint=\"/v1/chat/completions\",\n",
        "  completion_window=\"24h\",\n",
        ")\n",
        "\n",
        "batch_job_id = batch_response.id\n",
        "print(batch_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xC4TzFZmZ-eL",
        "outputId": "ee6f434a-06d6-4f1a-fa37-a40d5ed6ac6d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch(id='batch_jrzxp7dWTniijDIAgfMuNJ8B', completion_window='24h', created_at=1721639535, endpoint='/v1/chat/completions', input_file_id='file-w6JIa9w7fF4B4YIqgZzw3GGH', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1721725935, failed_at=None, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_file_id = client.batches.retrieve(batch_job_id).output_file_id\n",
        "status = client.batches.retrieve(batch_job_id).status\n",
        "print(f\"Job status: {status}\")\n",
        "print(f\"Output file id: {output_file_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPs8jk0MbIWt",
        "outputId": "028f7876-0691-4692-946c-95530a115cf9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job status: completed\n",
            "Output file id: file-9foa6xlpeg8BjutdzEx5k7Ze\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieveing the results"
      ],
      "metadata": {
        "id": "risj3_5R8Jnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = client.files.content(output_file_id)\n",
        "tagged_texts = []\n",
        "\n",
        "# Iterate over the response content line by line\n",
        "for line in output_file.iter_lines():\n",
        "    try:\n",
        "        response = json.loads(line)\n",
        "        data = response['response']['body']['choices'][0]['message']['content']\n",
        "        tagged_texts.append(data)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Skipping non-JSON line:\", line)"
      ],
      "metadata": {
        "id": "FwazjucFlBJY"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(batch_group[batch_name][51])\n",
        "print(\"\\n\")\n",
        "print(tagged_texts[51])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7Xl3U6Fg4s1",
        "outputId": "39ca4ea5-1e74-435c-8230-920aa91bb35d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El art√≠culo describe la isla de Sinjido, ubicada en la provincia de Jeollanam-do, Corea del Sur. Con una extensi√≥n de 30,99 km cuadrados, Sinjido cuenta con varios cerros como Sang-san, Nohak-bong y Beom-san. La isla se conecta con el continente a trav√©s de un puente construido en 2004 y tiene una longitud de 13 km.\n",
            "\n",
            "\n",
            "isla de Sinjido <> Isla <> Porci√≥n de tierra rodeada de agua.  \n",
            "Jeollanam-do <> Provincia <> Divis√£o administrativa de Corea del Sur.  \n",
            "Corea del Sur <> Pa√≠s <> Naci√≥n ubicada en el este de Asia.  \n",
            "30,99 km cuadrados <> Medida de superficie <> Unidad de medida que indica √°rea.  \n",
            "Sang-san <> Cerro <> Elevaci√≥n natural del terreno.  \n",
            "Nohak-bong <> Cerro <> Elevaci√≥n natural del terreno.  \n",
            "Beom-san <> Cerro <> Elevaci√≥n natural del terreno.  \n",
            "puente <> Infraestructura <> Estructura que permite la comunicaci√≥n entre dos puntos separados por un obst√°culo.  \n",
            "2004 <> A√±o <> Periodo de tiempo que indica el a√±o 2004.  \n",
            "13 km <> Longitud <> Medida que indica la extensi√≥n en l√≠nea recta entre dos puntos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs = batch_group[batch_name]\n",
        "if len(tagged_texts) == len(paragraphs):\n",
        "  dataset = []\n",
        "  for tagged_text, paragraph in zip(tagged_texts, paragraphs):\n",
        "    # Remove spaces at the end of each line and filter empty lines\n",
        "    lines = tagged_text.split('\\n')\n",
        "    ner_tags = [line.strip() for line in lines if line.strip()]\n",
        "\n",
        "    dataset.append({\n",
        "        \"input\": paragraph,\n",
        "        \"output\": ner_tags\n",
        "    })\n",
        "else:\n",
        "  print(\"The number of tagged texts and originals paragraphs are different.\")"
      ],
      "metadata": {
        "id": "zAVbGgZls1Xn"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_text_by_concatenating_content(texts, filepath):\n",
        "  with open(filepath, \"a\", encoding='utf-8') as json_file:\n",
        "    for text in texts:\n",
        "        json_line = json.dumps(text, ensure_ascii=False)\n",
        "        json_file.write(json_line + '\\n')\n",
        "  print(f\"Saved {len(texts)} texts\")"
      ],
      "metadata": {
        "id": "xwKGn0qfffg-"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_text_by_concatenating_content(dataset, f\"{BASE_PATH}/ner_dataset.jsonl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVBnB85lfqCa",
        "outputId": "973dc6d8-f51a-492b-b7c4-530281c4f383"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 469 texts\n"
          ]
        }
      ]
    }
  ]
}