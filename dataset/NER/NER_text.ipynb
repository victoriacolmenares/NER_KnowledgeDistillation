{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "D3LhGvjpmsIU"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiqeT2aCtnGf",
        "outputId": "b3bc3d40-a43b-4a3a-c82d-1358bdb2aabd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "HRIb4XYAk8Fo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92251395-8388-41c4-c85d-f650fca9eb5c",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.36.1-py3-none-any.whl (328 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/328.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m256.0/328.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.8/328.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.36.1\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import tiktoken\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "A2YaDedCk5Bc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab/NER\""
      ],
      "metadata": {
        "id": "Em2Uw2dRmFBB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate price using tiktoken"
      ],
      "metadata": {
        "id": "D3LhGvjpmsIU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get total paragraphs"
      ],
      "metadata": {
        "id": "rizCuHfvtStB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        return file.read()"
      ],
      "metadata": {
        "id": "mEfj1ERdtVsZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = f\"{BASE_PATH}/wikipedia_summary.txt\"\n",
        "text = read_file(filepath)\n",
        "paragraphs = text.replace(\"\\n\\n\", \"\\n\").split(\"\\n\")\n",
        "print(f\"The file contains {len(paragraphs)} paragraphs.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98qI6VY4tbL6",
        "outputId": "56332d5a-a652-402b-d8ad-c7a8dda31ecc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file contains 110038 paragraphs.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate price\n",
        "\n",
        "[Pricing OpenAI](https://openai.com/api/pricing/)"
      ],
      "metadata": {
        "id": "L5mhVwa9u0eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pricing in USD\n",
        "model_pricing_by_1K_tokens = {\n",
        "  \"gpt-4o-mini\": {\n",
        "    \"input_tokens\": {\n",
        "        \"api\": 0.00015,\n",
        "        \"bacth_api\": 0.00008\n",
        "    },\n",
        "    \"output_tokens\": {\n",
        "        \"api\": 0.0006,\n",
        "        \"bacth_api\": 0.0003\n",
        "    }\n",
        "  },\n",
        "  \"gpt-4o\": {\n",
        "    \"input_tokens\": {\n",
        "        \"api\": 0.005,\n",
        "        \"bacth_api\": 0.0025\n",
        "    },\n",
        "    \"output_tokens\": {\n",
        "        \"api\": 0.015,\n",
        "        \"bacth_api\": 0.0075\n",
        "    }\n",
        "  },\n",
        "}\n"
      ],
      "metadata": {
        "id": "j2RITvlVpChN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens_per_text(text, model):\n",
        "  try:\n",
        "      encoding = tiktoken.encoding_for_model(model)\n",
        "  except KeyError:\n",
        "      print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
        "      encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "  token_integers = encoding.encode(text)\n",
        "  print(f\"Model: {model}\\nEncoding: {encoding.name}\")\n",
        "  return len(token_integers)"
      ],
      "metadata": {
        "id": "xUQW594snGZa"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_cost(num_tokens, fee_per_token, tokens=1000):\n",
        "  final_price = (num_tokens * fee_per_token) / tokens\n",
        "  return final_price"
      ],
      "metadata": {
        "id": "Fqfx__gfnJT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gpt-4o-mini\"\n",
        "model_pricing = model_pricing_by_1K_tokens[model]\n",
        "batch_format_sample = '{\"custom_id\": \"wk-499\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-4o-mini\", \"messages\": [{\"role\": \"system\", \"content\": \"Eres un experto generando y creando etiquetas para un texto en español.\"}, {\"role\": \"user\", \"content\": \"Como especialista anotador de información, tu tarea es detectar entidades de todo tipo en un texto para generar un dataset de entrenamiento de un modelo de IA.\\nEtiqueta todo tipo de entidades, conceptos e ideas como te sea posible, saliéndote del esquema general de los datasets NER.Sientete libre de crear nuevos tipos de entidades que pueden no existir en tareas NER tradicionales.\\nEl formato de salida es el siguiente: <palabra> <> <nombre de la entidad> <> <descripcion de la entidad>\\nEjemplo: España <> País <> Nación localizada en el sur de EuropaEl texto es el siguiente: Elvira Malagarriga Armart, también conocida como Elvira Ormart, fue una pintora española especializada en retratos, paisajes y flores. Nacida en Barcelona en 1886, estudió en la Escuela de Bellas Artes de La Llotja y luego se formó en París, donde asistió a la Académie Julian. Malagarriga expuso su obra en diversos lugares de España y Francia, incluyendo el Salón de París, la Asociación de Artistas Italianos en Florencia, y en Sala Parés en Barcelona. Su obra incluye retratos, como el de su padre y su hermana, así como paisajes y flores. Su estilo fue elogiado por la revista Feminal, que comparó su habilidad con la de otras pintoras femeninas de la época. El Museo Nacional de Arte de Cataluña conserva una de sus obras, un paseo con una casa señorial.\"}], \"max_tokens\": 2500}}'\n",
        "\n",
        "if model_pricing:\n",
        "  num_tokens = num_tokens_per_text(batch_format_sample, model)\n",
        "  print(f\"The text contains: {num_tokens} tokens.\")\n",
        "\n",
        "  # Estimate cost\n",
        "  input_price_with_api = estimate_cost(num_tokens, model_pricing[\"input_tokens\"][\"api\"])\n",
        "  print(f\"Estimated cost with API per paragraph: {input_price_with_api:2f}$.\")\n",
        "\n",
        "  input_price_with_api_per_100k_texts = input_price_with_api * len(paragraphs)\n",
        "  print(f\"Estimated cost with API per {len(paragraphs)} paragraphs: {input_price_with_api_per_100k_texts:2f}$.\")\n",
        "\n",
        "  input_price_with_batch_api = estimate_cost(num_tokens, model_pricing[\"input_tokens\"][\"bacth_api\"])\n",
        "  print(f\"Estimated cost with Batch API per paragraph: {input_price_with_batch_api:2f}$.\")\n",
        "\n",
        "  input_price_with_batch_api_per_100k_texts = input_price_with_batch_api * len(paragraphs)\n",
        "  print(f\"Estimated cost with Batch API per {len(paragraphs)} paragraphs: {input_price_with_batch_api_per_100k_texts:2f}$.\")\n",
        "else:\n",
        "  print(f\"Model {model} not found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1JPgLaAnMPf",
        "outputId": "65c35319-bf24-4a9c-c799-3065a2b50ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "The text contains: 396 tokens.\n",
            "Estimated cost with API per paragraph: 0.000059$.\n",
            "Estimated cost with API per 110038 paragraphs: 6.536257$.\n",
            "Estimated cost with Batch API per paragraph: 0.000032$.\n",
            "Estimated cost with Batch API per 110038 paragraphs: 3.486004$.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4XNbcXCuFb85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate NER tags"
      ],
      "metadata": {
        "id": "3sfwXohaBQvz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create promt for generate NER tags"
      ],
      "metadata": {
        "id": "os9FOqTP8QWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Run a test for the prompt***"
      ],
      "metadata": {
        "id": "iDLj6iM3BnR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = paragraphs[45]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  messages=[\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"Eres un experto generando y creando etiquetas para un texto en español.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": (\n",
        "            \"Como especialista anotador de información, tu tarea es detectar entidades de todo tipo en un texto para generar un dataset de entrenamiento de un modelo de IA.\\n\"\n",
        "            \"Etiqueta todo tipo de entidades, conceptos e ideas como te sea posible, saliéndote del esquema general de los datasets NER.\"\n",
        "            \"Sientete libre de crear nuevos tipos de entidades que pueden no existir en tareas NER tradicionales.\\n\"\n",
        "            \"Muy importante: NO etiquetes palabras comunes como artículos ('El', 'La', 'Los', 'Las'). En su lugar, etiqueta la palabra o frase a la que hace referencia el artículo.\\n\"\n",
        "            \"El formato de salida es el siguiente: <palabra> <> <nombre de la entidad> <> <descripcion de la entidad>\"\n",
        "            \"Ejemplo: España <> País <> Nación localizada en el sur de Europa.\"\n",
        "            f\"El texto es el siguiente: {sample_text}\"\n",
        "        ),\n",
        "    },\n",
        "  ],\n",
        "  temperature=0.7,\n",
        "  max_tokens=1024,\n",
        "  top_p=0.7,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0\n",
        ")\n",
        "\n",
        "response.choices[0].message.content.replace(\"\\n\\n\", \"\\n\").split(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7vPdNw88dza",
        "outputId": "7027b05b-0d47-4f4d-81c4-db05b8d04d99"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['álbum <> Obra musical <> Colección de canciones grabadas por un artista o banda.  ',\n",
              " 'En Directo <> Título de álbum <> Nombre del álbum grabado por la banda Cicatriz.  ',\n",
              " 'banda <> Grupo musical <> Conjunto de músicos que tocan juntos.  ',\n",
              " 'Cicatriz <> Banda de rock <> Grupo musical de rock originario de España.  ',\n",
              " '1994 <> Año <> Año en el que fue grabado el álbum.  ',\n",
              " '1995 <> Año <> Año en el que fue lanzado el álbum.  ',\n",
              " 'homenaje <> Concepto <> Acto de recordar y honrar a alguien.  ',\n",
              " 'miembros <> Personas <> Integrantes de la banda Cicatriz.  ',\n",
              " 'fallecidos <> Estado <> Personas que han muerto.  ',\n",
              " 'energía <> Concepto <> Vitalidad y fuerza que transmite la música.  ',\n",
              " 'sonido metalero <> Género musical <> Estilo musical caracterizado por guitarras eléctricas distorsionadas y ritmos intensos.  ',\n",
              " 'guitarrista <> Músico <> Persona que toca la guitarra.  ',\n",
              " 'Goar Iñurrieta <> Guitarrista <> Nombre del guitarrista de la banda Cicatriz.  ',\n",
              " 'estilo heavy <> Estilo musical <> Subgénero del rock caracterizado por su sonido fuerte y agresivo.  ',\n",
              " 'canciones punk <> Género musical <> Composiciones musicales del estilo punk, caracterizadas por su energía y actitud rebelde.  ',\n",
              " 'intensidad <> Concepto <> Grado de fuerza y emoción en las actuaciones.  ',\n",
              " 'actuaciones en vivo <> Evento musical <> Presentaciones en directo ante un público.  ',\n",
              " 'formación original <> Estructura <> Conjunto inicial de miembros de la banda Cicatriz.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Batch API"
      ],
      "metadata": {
        "id": "szLeWrLYzaZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Batch file\n",
        "- Create a Jsonl file.\n",
        "- Each request must include a unique custom_id value, which you can use to reference results after completion\n",
        "\n",
        "Example:\n",
        "\n",
        "\n",
        "```\n",
        "{\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-3.5-turbo-0125\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},{\"role\": \"user\", \"content\": \"Hello world!\"}],\"max_tokens\": 1000}}\n",
        "{\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-3.5-turbo-0125\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an unhelpful assistant.\"},{\"role\": \"user\", \"content\": \"Hello world!\"}],\"max_tokens\": 1000}}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ANvwfsiB7psv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare global variables\n",
        "method = \"POST\"\n",
        "url = \"/v1/chat/completions\"\n",
        "model = \"gpt-4o-mini\"\n",
        "max_tokens = 2500\n",
        "system_prompt = \"Eres un experto generando y creando etiquetas para un texto en español.\"\n",
        "chunk_size = 500\n",
        "limit_token = 200000\n",
        "batch_group = {}"
      ],
      "metadata": {
        "id": "RU6DdVSLFOIh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_text_in_jsonl(texts, file_path):\n",
        "  with open(file_path, \"w\", encoding='utf-8') as json_file:\n",
        "    for text in texts:\n",
        "        json_line = json.dumps(text, ensure_ascii=False)\n",
        "        json_file.write(json_line + '\\n')"
      ],
      "metadata": {
        "id": "hTOk1yyoJQrP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(paragraphs), chunk_size):\n",
        "  batch_format = []\n",
        "  chunk = paragraphs[i: i + chunk_size]\n",
        "  batch_name = f\"batch_{i}\"\n",
        "  batch_group[batch_name] = []\n",
        "\n",
        "  for index, paragraph in enumerate(chunk):\n",
        "    # To exclude phrases that are not article summaries\n",
        "    if len(paragraph) > 70:\n",
        "      batch_group[batch_name].append(paragraph)\n",
        "      custom_id = f\"wk-{index}\"\n",
        "      message = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_prompt\n",
        "        },\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": (\n",
        "            \"Como especialista anotador de información, tu tarea es detectar entidades de todo tipo en un texto para generar un dataset de entrenamiento de un modelo de IA.\\n\"\n",
        "            \"Etiqueta todo tipo de entidades, conceptos e ideas como te sea posible, saliéndote del esquema general de los datasets NER.\"\n",
        "            \"Sientete libre de crear nuevos tipos de entidades que pueden no existir en tareas NER tradicionales.\\n\"\n",
        "            \"Muy importante: NO etiquetes palabras comunes como artículos ('El', 'La', 'Los', 'Las'). En su lugar, etiqueta la palabra o frase a la que hace referencia el artículo.\\n\"\n",
        "            \"El formato de salida es el siguiente: <palabra> <> <nombre de la entidad> <> <descripcion de la entidad>\"\n",
        "            \"Ejemplo: España <> País <> Nación localizada en el sur de Europa.\"\n",
        "            f\"\\nEl texto es el siguiente: {paragraph}\"\n",
        "          )\n",
        "        }\n",
        "      ]\n",
        "      batch_format.append({\n",
        "        \"custom_id\": custom_id,\n",
        "        \"method\": method,\n",
        "        \"url\": url,\n",
        "        \"body\": {\n",
        "          \"model\": model,\n",
        "          \"messages\": message,\n",
        "          \"max_tokens\": max_tokens\n",
        "        }\n",
        "      })\n",
        "\n",
        "  # total_tokens = num_tokens_per_text(str(batch_format), model)\n",
        "  # if total_tokens > limit_token:\n",
        "  #   print(f\"💥The token amount is greater than that allowed by {model}. Total tokens: {total_tokens}\")\n",
        "  #   break\n",
        "\n",
        "  filename = f\"batch_{i}.jsonl\"\n",
        "  filepath = f\"{BASE_PATH}/{filename}\"\n",
        "\n",
        "  save_text_in_jsonl(batch_format, filepath)\n",
        "  print(f\"Saved file: {filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HupG7Ck7hYi",
        "outputId": "2383af64-d741-4055-f557-13a206b04c2d",
        "collapsed": true
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_0.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_1000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_1500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_2000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_2500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_3000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_3500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_4000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_4500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_5000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_5500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_6000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_6500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_7000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_7500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_8000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_8500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_9000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_9500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_10000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_10500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_11000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_11500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_12000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_12500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_13000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_13500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_14000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_14500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_15000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_15500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_16000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_16500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_17000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_17500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_18000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_18500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_19000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_19500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_20000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_20500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_21000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_21500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_22000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_22500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_23000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_23500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_24000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_24500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_25000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_25500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_26000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_26500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_27000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_27500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_28000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_28500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_29000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_29500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_30000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_30500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_31000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_31500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_32000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_32500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_33000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_33500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_34000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_34500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_35000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_35500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_36000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_36500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_37000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_37500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_38000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_38500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_39000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_39500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_40000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_40500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_41000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_41500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_42000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_42500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_43000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_43500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_44000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_44500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_45000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_45500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_46000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_46500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_47000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_47500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_48000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_48500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_49000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_49500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_50000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_50500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_51000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_51500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_52000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_52500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_53000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_53500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_54000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_54500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_55000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_55500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_56000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_56500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_57000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_57500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_58000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_58500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_59000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_59500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_60000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_60500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_61000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_61500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_62000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_62500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_63000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_63500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_64000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_64500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_65000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_65500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_66000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_66500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_67000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_67500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_68000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_68500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_69000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_69500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_70000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_70500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_71000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_71500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_72000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_72500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_73000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_73500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_74000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_74500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_75000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_75500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_76000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_76500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_77000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_77500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_78000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_78500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_79000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_79500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_80000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_80500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_81000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_81500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_82000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_82500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_83000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_83500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_84000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_84500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_85000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_85500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_86000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_86500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_87000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_87500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_88000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_88500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_89000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_89500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_90000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_90500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_91000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_91500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_92000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_92500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_93000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_93500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_94000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_94500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_95000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_95500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_96000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_96500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_97000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_97500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_98000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_98500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_99000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_99500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_100000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_100500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_101000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_101500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_102000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_102500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_103000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_103500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_104000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_104500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_105000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_105500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_106000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_106500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_107000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_107500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_108000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_108500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_109000.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_109500.jsonl\n",
            "Model: gpt-4o-mini\n",
            "Encoding: o200k_base\n",
            "Saved file: batch_110000.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload Batch file"
      ],
      "metadata": {
        "id": "UJPcEc537v1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jsonl_files = [\n",
        "  f\"{BASE_PATH}/{file}\"\n",
        "  for file in os.listdir(BASE_PATH)\n",
        "  if file.endswith(\".jsonl\")\n",
        "]\n",
        "len(jsonl_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6yl8oY0PUwEf",
        "outputId": "8dccbfeb-b79e-42e8-c7ca-f07d99bd66a8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "222"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Do the following steps for the number of jsonl files"
      ],
      "metadata": {
        "id": "nnki5Wp3bFCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_name = \"batch_0\"\n",
        "jsonl_file = f\"{batch_name}.jsonl\""
      ],
      "metadata": {
        "id": "oCtquhZzrq4w"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_file = client.files.create(\n",
        "  file=open(f\"{BASE_PATH}/{jsonl_file}\", \"rb\"),\n",
        "  purpose=\"batch\"\n",
        ")\n",
        "\n",
        "batch_file_id = batch_file.id\n",
        "print(f\"Uploaded: {jsonl_file} \\nFile id: {batch_file_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlAqVcTzUWQo",
        "outputId": "c18ac57d-ad7b-4be2-f1c7-e47dd6bd80f2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded: batch_0.jsonl \n",
            "File id: file-w6JIa9w7fF4B4YIqgZzw3GGH\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the batch job\n",
        "💡One job must be created at a time"
      ],
      "metadata": {
        "id": "nrjJ7vJl7udM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_response = client.batches.create(\n",
        "  input_file_id=batch_file_id,\n",
        "  endpoint=\"/v1/chat/completions\",\n",
        "  completion_window=\"24h\",\n",
        ")\n",
        "\n",
        "batch_job_id = batch_response.id\n",
        "print(batch_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xC4TzFZmZ-eL",
        "outputId": "ee6f434a-06d6-4f1a-fa37-a40d5ed6ac6d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch(id='batch_jrzxp7dWTniijDIAgfMuNJ8B', completion_window='24h', created_at=1721639535, endpoint='/v1/chat/completions', input_file_id='file-w6JIa9w7fF4B4YIqgZzw3GGH', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1721725935, failed_at=None, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_file_id = client.batches.retrieve(batch_job_id).output_file_id\n",
        "status = client.batches.retrieve(batch_job_id).status\n",
        "print(f\"Job status: {status}\")\n",
        "print(f\"Output file id: {output_file_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPs8jk0MbIWt",
        "outputId": "028f7876-0691-4692-946c-95530a115cf9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job status: completed\n",
            "Output file id: file-9foa6xlpeg8BjutdzEx5k7Ze\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieveing the results"
      ],
      "metadata": {
        "id": "risj3_5R8Jnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = client.files.content(output_file_id)\n",
        "tagged_texts = []\n",
        "\n",
        "# Iterate over the response content line by line\n",
        "for line in output_file.iter_lines():\n",
        "    try:\n",
        "        response = json.loads(line)\n",
        "        data = response['response']['body']['choices'][0]['message']['content']\n",
        "        tagged_texts.append(data)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Skipping non-JSON line:\", line)"
      ],
      "metadata": {
        "id": "FwazjucFlBJY"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(batch_group[batch_name][51])\n",
        "print(\"\\n\")\n",
        "print(tagged_texts[51])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7Xl3U6Fg4s1",
        "outputId": "39ca4ea5-1e74-435c-8230-920aa91bb35d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El artículo describe la isla de Sinjido, ubicada en la provincia de Jeollanam-do, Corea del Sur. Con una extensión de 30,99 km cuadrados, Sinjido cuenta con varios cerros como Sang-san, Nohak-bong y Beom-san. La isla se conecta con el continente a través de un puente construido en 2004 y tiene una longitud de 13 km.\n",
            "\n",
            "\n",
            "isla de Sinjido <> Isla <> Porción de tierra rodeada de agua.  \n",
            "Jeollanam-do <> Provincia <> Divisão administrativa de Corea del Sur.  \n",
            "Corea del Sur <> País <> Nación ubicada en el este de Asia.  \n",
            "30,99 km cuadrados <> Medida de superficie <> Unidad de medida que indica área.  \n",
            "Sang-san <> Cerro <> Elevación natural del terreno.  \n",
            "Nohak-bong <> Cerro <> Elevación natural del terreno.  \n",
            "Beom-san <> Cerro <> Elevación natural del terreno.  \n",
            "puente <> Infraestructura <> Estructura que permite la comunicación entre dos puntos separados por un obstáculo.  \n",
            "2004 <> Año <> Periodo de tiempo que indica el año 2004.  \n",
            "13 km <> Longitud <> Medida que indica la extensión en línea recta entre dos puntos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs = batch_group[batch_name]\n",
        "if len(tagged_texts) == len(paragraphs):\n",
        "  dataset = []\n",
        "  for tagged_text, paragraph in zip(tagged_texts, paragraphs):\n",
        "    # Remove spaces at the end of each line and filter empty lines\n",
        "    lines = tagged_text.split('\\n')\n",
        "    ner_tags = [line.strip() for line in lines if line.strip()]\n",
        "\n",
        "    dataset.append({\n",
        "        \"input\": paragraph,\n",
        "        \"output\": ner_tags\n",
        "    })\n",
        "else:\n",
        "  print(\"The number of tagged texts and originals paragraphs are different.\")"
      ],
      "metadata": {
        "id": "zAVbGgZls1Xn"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_text_by_concatenating_content(texts, filepath):\n",
        "  with open(filepath, \"a\", encoding='utf-8') as json_file:\n",
        "    for text in texts:\n",
        "        json_line = json.dumps(text, ensure_ascii=False)\n",
        "        json_file.write(json_line + '\\n')\n",
        "  print(f\"Saved {len(texts)} texts\")"
      ],
      "metadata": {
        "id": "xwKGn0qfffg-"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_text_by_concatenating_content(dataset, f\"{BASE_PATH}/ner_dataset.jsonl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVBnB85lfqCa",
        "outputId": "973dc6d8-f51a-492b-b7c4-530281c4f383"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 469 texts\n"
          ]
        }
      ]
    }
  ]
}